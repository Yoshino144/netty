\---
title: Netty 内存管理 Jemalloc3 源码解析
date: 2023-05-18 23:30:48
tags:
  \- Java
  \- Netty
  \- 网络编程
categories:
  \- Netty
top_img: bg.jpg
cover: bg.jpg
copyright_author: pc
copyright_author_href: http://pcat.top
copyright_url: http://pcat.top/2023/05/18/netty-m-j4/
copyright_info: 此文章版权归pcat所有，如有转载，请注明来自原作者
sticky: 1
description:  
\---

# Netty内存管理 Jemalloc3

## 概述

`Netty 4.1.45`以前采用`jemalloc3`算法 实现内存分配，而`Netty 4.1.45`之后使用`jemalloc4`算法分配内存。

### 高性能内存分配

​		jemalloc 是由 Jason Evans 在 FreeBSD 项目中引入的新一代内存分配器。它是一个通用的 malloc 实现，侧重于减少内存碎片和提升高并发场景下内存的分配效率，其目标是能够替代 malloc。

​		jemalloc 应用十分广泛，在 Firefox、Redis、Rust、Netty 等出名的产品或者编程语言中都有大量使用。具体细节可以参考 Jason Evans 发表的论文 [《A Scalable Concurrent malloc Implementation for FreeBSD》]。
除了 jemalloc 之外，业界还有一些著名的高性能内存分配器实现，比如 ptmalloc 和 tcmalloc。简单对比如下:

- ptmalloc（per-thread malloc） 基于 glibc 实现的内存分配器，由于是标准实现，兼容性较好。缺点是多线程之间内存无法实现共享，内存开销很大。
- tcmalloc（thread-caching malloc） 是由 Google 开源，最大特点是带有线程缓存，目前在 Chrome、Safari 等产品中有所应用。tcmalloc 为每个线程分配一个局部缓存，可以从线程局部缓冲分配小内存对象，而对于大内存分配则使用自旋锁减少内存竞争，提高内存效率。
- jemalloc 借鉴 tcmalloc 优秀的设计思路，所以在架构设计方面两者有很多相似之处，同样都包含线程缓存特性。但是 jemalloc 在设计上比 tcmalloc 要复杂。它将内存分配粒度划分为** Small、Large、Huge**，并记录了很多元数据，所以元数据占用空间高于 tcmalloc。

## 内存碎片

​		在 Linux 世界，物理内存会被划分成若干个 4KB 大小的内存页(page)，这是分配内存大小的最小粒度。分配和回收都是基于 page 完成的。page 内产生的碎片称为 内存碎片，page 外产生的碎片称为 外部碎片。
​		内存碎片产生的原因是内存被分割成很小的块，虽然这些块是空闲且地址连续的，但却小到无法使用。随着内存的分配和释放次数的增加，内存将变得越来越不连续。最后，整个内存将只剩下碎片，即便有足够的空闲页框可以满足请求，但要分配一个大块的连续页框就无法满足，所以减少内存浪费的核心就是尽量避免产生内存碎片。

## 内存规格

​		Netty 保留了对不同大小的内存采用不同的分配策略，具体规格如上图所示。在 Netty 中定义了 `io.netty.buffer.PoolArena.SizeClass` 枚举类，用于描述上图的内存规格类型，分别是 Tiny、Small 和 Normal。当 >16MB 时，归为Huge类型。Netty 在每个区域内又定义了更细粒度的内存分配单位，分别是 Chunk、Page 和 Subpage。

![在这里插入图片描述](./assets/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0NsYXJlbmNlWmVybw==,size_16,color_FFFFFF,t_70.png)

## 内存规格化

对于 Huge 级别的内存大小，用户申请多少内存就返回多少内存（如有必要，需要内存对齐）。

对于 tiny 、small 、normal 级别的内存，以 512B 为分界线有:

- 当 >=512B时，返回最接近 2且大于用户申请内存的大小的值。比如申请内存大小为 513B，则返回 1024B。

- 当 <512B 时，返回最接近 16 的倍数且大于用户申请内存的大小的值。比如申请内存大小为 17B，则返回 32B; 申请内存大小为 46B，返回 48B。

**内存规格化**源码在 `io.netty.buffer.PoolArena` 对象中:

```java
int normalizeCapacity(int reqCapacity) {
    //检查用户输入的内存大小，必须是大于0的数值
    checkPositiveOrZero(reqCapacity, "reqCapacity");
    //大于16M的，如果指定了校准值，那么使用校准值进行校准，如果没有指定，直接返回
    if (reqCapacity >= chunkSize) {
        return directMemoryCacheAlignment == 0 ? reqCapacity : alignCapacity(reqCapacity);
    }
    //处理大于等于512字节的内存
    if (!isTiny(reqCapacity)) { // >= 512
        // Doubled
        //修正为最接近reqCapacity的2的n次幂的值，原理就是将reqCapacity中最高bit位为1以下的bit位统统变成1
        //最后在加1就得到了最近的2的n次幂的值，由于最高为1的bit位不确定是第几位，所以这里迁移了31次
        int normalizedCapacity = reqCapacity;
        normalizedCapacity --;//减一是因为有可能当前reqCapacity就是一个2的n次幂的值
        normalizedCapacity |= normalizedCapacity >>>  1;//对于一个第32个bit为1的值来说，向右移动一位，至少可以得到2个1
        normalizedCapacity |= normalizedCapacity >>>  2;//因为上一步至少得到了2个1，那么此时我可以移动2位，得到了4个1
        normalizedCapacity |= normalizedCapacity >>>  4;//因为上一步至少得到了4个1，那么我可以移4位，这样就可以至少得到8个1
        normalizedCapacity |= normalizedCapacity >>>  8;//因为上一步至少得到了8个1，那么我可以移8位，这样就可以至少得到16个1
        normalizedCapacity |= normalizedCapacity >>> 16;//因为上一步至少得到了16个1，那么我可以移16位，这样就可以至少得到32个1
        normalizedCapacity ++;
        //如果溢出，无符号右移，变成整数
        if (normalizedCapacity < 0) {
            normalizedCapacity >>>= 1;
        }
        assert directMemoryCacheAlignment == 0 || (normalizedCapacity & directMemoryCacheAlignmentMask) == 0;

        return normalizedCapacity;
    }
    //是否需要校准
    if (directMemoryCacheAlignment > 0) {
        return alignCapacity(reqCapacity);
    }

    // Quantum-spaced
    //对16求余，如果没有余数，那么说明可以整除16，它是16的倍数，可以直接返回
    if ((reqCapacity & 15) == 0) {
        return reqCapacity;
    }
    //如果reqCapacity比16小，那么reqCapacity & ~15将会返回0
    //~15 低4位全是0，高28位全是1，所以如果一个值小于16，那么 reqCapacity & ~15 等于0
    //对于大于16的，进行 reqCapacity & ~15 之后低4全部置零，而高28位，不管哪个bit为是1，都是16的倍数
    return (reqCapacity & ~15) + 16;
}

```

## 内存分配过程

### jemalloc3 算法逻辑

1. 首先，Netty 会向 操作系统 申请一整块 **连续内存，**称为 chunk（数据块），除非申请 Huge 级别大小的内存，否则一般大小为 16MB，使用 io.netty.buffer.PoolChunk 对象包装。具体长这样子:

2. Netty将chunk进一步拆分为多个page，每个 page 默认大小为 8KB，因此每个 chunk 包含 2048 个 page。为了对小内存进行精细化管理，减少内存碎片，提高内存使用率，Netty 对 **page **进一步拆分若干 subpage，subpage 的大小是动态变化的，最小为 16Byte。
3. 计算: 当请求内存分配时，将所需要内存大小进行内存规格化，获得合适的内存值。根据值确认准确的树的高度。
4. 搜索: 在该分组大小的相应高度中从左至右搜寻空闲分组并进行分配。
5. 标记: 分组被标记为全部已使用，且通过循环更新其父节点标记信息。父节点的标记值取两个子节点标记值的最小的一个。

### Huge 分配逻辑概述

​		大内存分配比其他类型的内存分配稍微简单一点，操作的内存单元是 PoolChunk，它的容量大小是用户申请的容量（可满足内存对齐要求）。Netty 对 Huge 对象的内存块采用非池化管理策略，在每次请求分配内存时单独创建特殊的非池化 PoolChunk 对象，当对象内存释放时整个 PoolChunk 内存也会被释放。
大内存的分配逻辑是在 io.netty.buffer.PoolArena#allocateHuge 完成。

### Normal 分配逻辑

​		Normal 级别分配的大小范围是 [4097B, 16M) 。核心思想是将 PoolChunk 拆分成 2048 个 page ，这是 Normal 分配的最小单位。每个 page 等大（pageSize=8KB），并在逻辑上通过一棵满二叉树管理这些 page 对象。我们申请的内存本质是组合若干个 page 。
Normal 的分配核心逻辑是在 PoolChunk#allocateRun(int) 完成。

### Small 分配逻辑

​		Small 级别分配的大小范围是 (496B, 4096B] 。核心是把一个 page 拆分若干个 Subpage，PoolSubpage 就是这些若干个 Subpage 的化身，有效解决小内存场景造成内存碎片的问题。
一个 page 大小为 8192B，有且只有四种大小: 512B、1024B、2048B 和 4096B，以 2 倍递增。当申请的内存大小在 496B~4096B 范围内时，就能确定这四种中的一种。
当进行内存分配时，先在树的最底层找到一个空闲的 page，拆分成若干个 subpage，并构造一个 PoolSubpage 进行管理。选择第一个 subpage 用于此次申请，标记为已使用，并将 PoolSubpage 放置在 PoolSubpage[] smallSubpagePools 数组所对应的链表中。等下次申请等大容量内存时就可从 PoolSubpage[] 直接分配从链表中分配内存。

### Tiny 分配逻辑 - j4算法没有

​		Tiny 级别分配的大小范围是 (0B, 496B] 。分配逻辑与 Small 类似，先找到空闲的 Page 然后将其拆分若干个 Subpage 并构造一个 PoolSubpage 对它们进行管理。随后选择第一个 subpage 用于此次申请，并将对象 PoolSubpage 放置在 PoolSubpage[] tinySubpagePools 数组所对应的链表中。等待下次分配时使用。区别在于如何定义若干个? Tiny 给出的定义逻辑是获取最接近 16*N 的且大于规格值的大小。比如申请内存大小为 31B，找到最接近的下一个 16*1 的倍数且大于 31 的值是 32，因此，就把 Page 拆分成 8192/32=256 个 subpage，这里的若干个就是根据规格值确定的，它是可变的值。

## PoolArena

​		PoolArena 是进行池化内存分配的核心类，采用固定数量的多个 Arena 进行内存分配，默认与 CPU 核心数量有关，它是线程共享的对象，每个线程只会绑定一个 PoolArena，线程和 PoolArena 是多对一的关系。当某个线程首次申请内存分配时，会通过轮询（Round-Robin） 方式得到一个 Arena，在该线程的整个生命周期内只和这个 Arena 打交道，前面也说过，PoolArena 是分治思想的体现，在多线程场景下有出色的表现。PoolArena 提供 DirectArena 和 HeapArena 子类，这是因为底层容器类型不同所以需要子类区分。但核心逻辑是在 PoolArena 完成的。

​		PoolArena 的数据结构大致（除去监测指标数据）可分为两大类: 存储 PoolChunk 的 6 个 PoolChunkList 和 存储 PoolSubpage 的 2 个数组。PoolArena 构造器初始化也做了很多重要的工作，包含串联 PoolChunkList 以及初始化 PoolSubpage[] 。

### 变量和构造器源码

```java
static final boolean HAS_UNSAFE = PlatformDependent.hasUnsafe();

// 三种不同 内存逻辑
enum SizeClass {
    Tiny,
    Small,
    Normal
}

// tiny数组长度
static final int numTinySubpagePools = 512 >>> 4;

/**
 *
 */
final PooledByteBufAllocator parent;

/**
 * 树的高度 默认11
 */
private final int maxOrder;
/**
 * 内存页大小， 8192
 */
final int pageSize;
/**
 * 页偏移量 13
 */
final int pageShifts;
/**
 * 内存块大小 16777216 16MB
 */
final int chunkSize;
/**
 * 溢出掩码
 */
final int subpageOverflowMask;
/**
 * Small数组数量  4
 */
final int numSmallSubpagePools;
/**
 * 直接内存对齐值 0
 */
final int directMemoryCacheAlignment;
/**
 * 直接内存对齐掩码
 */
final int directMemoryCacheAlignmentMask;
// 2个 PoolSubpage 数组
private final PoolSubpage<T>[] tinySubpagePools;
private final PoolSubpage<T>[] smallSubpagePools;
// 6 个 PoolChunkList
private final PoolChunkList<T> q050;
private final PoolChunkList<T> q025;
private final PoolChunkList<T> q000;
private final PoolChunkList<T> qInit;
private final PoolChunkList<T> q075;
private final PoolChunkList<T> q100;

private final List<PoolChunkListMetric> chunkListMetrics;

// Metrics for allocations and deallocations
// 分配和解除分配的指标
private long allocationsNormal;
// We need to use the LongCounter here as this is not guarded via synchronized block.
// 我们需要在这里使用LongCounter，因为这不是通过同步块保护的。
private final LongCounter allocationsTiny = PlatformDependent.newLongCounter();
private final LongCounter allocationsSmall = PlatformDependent.newLongCounter();
private final LongCounter allocationsHuge = PlatformDependent.newLongCounter();
private final LongCounter activeBytesHuge = PlatformDependent.newLongCounter();

// 释放次数
private long deallocationsTiny;
private long deallocationsSmall;
private long deallocationsNormal;

// We need to use the LongCounter here as this is not guarded via synchronized block.
// 我们需要在这里使用LongCounter，因为这不是通过同步块保护的。
private final LongCounter deallocationsHuge = PlatformDependent.newLongCounter();

// Number of thread caches backed by this arena.
// 此竞技场支持的线程缓存数。
final AtomicInteger numThreadCaches = new AtomicInteger();

// TODO: Test if adding padding helps under contention
//private long pad0, pad1, pad2, pad3, pad4, pad5, pad6, pad7;

// 构造器
protected PoolArena(PooledByteBufAllocator parent, int pageSize,
      int maxOrder, int pageShifts, int chunkSize, int cacheAlignment) {
    this.parent = parent;
    this.pageSize = pageSize;
    this.maxOrder = maxOrder;
    this.pageShifts = pageShifts;
    this.chunkSize = chunkSize;
    directMemoryCacheAlignment = cacheAlignment;
    directMemoryCacheAlignmentMask = cacheAlignment - 1;
    subpageOverflowMask = ~(pageSize - 1);
    // 创建 tinySubpagePools 数组
    tinySubpagePools = newSubpagePoolArray(numTinySubpagePools);
    for (int i = 0; i < tinySubpagePools.length; i ++) {
        tinySubpagePools[i] = newSubpagePoolHead(pageSize);
    }
    // 创建 numSmallSubpagePools 数组
    numSmallSubpagePools = pageShifts - 9;
    smallSubpagePools = newSubpagePoolArray(numSmallSubpagePools);
    for (int i = 0; i < smallSubpagePools.length; i ++) {
        smallSubpagePools[i] = newSubpagePoolHead(pageSize);
    }

    // 初始化 PoolChunkList
    q100 = new PoolChunkList<T>(this, null, 100, Integer.MAX_VALUE, chunkSize);
    q075 = new PoolChunkList<T>(this, q100, 75, 100, chunkSize);
    q050 = new PoolChunkList<T>(this, q075, 50, 100, chunkSize);
    q025 = new PoolChunkList<T>(this, q050, 25, 75, chunkSize);
    q000 = new PoolChunkList<T>(this, q025, 1, 50, chunkSize);
    qInit = new PoolChunkList<T>(this, q000, Integer.MIN_VALUE, 25, chunkSize);

    q100.prevList(q075);
    q075.prevList(q050);
    q050.prevList(q025);
    q025.prevList(q000);
    q000.prevList(null);
    qInit.prevList(qInit);

    // 监控信息
    List<PoolChunkListMetric> metrics = new ArrayList<PoolChunkListMetric>(6);
    metrics.add(qInit);
    metrics.add(q000);
    metrics.add(q025);
    metrics.add(q050);
    metrics.add(q075);
    metrics.add(q100);
    chunkListMetrics = Collections.unmodifiableList(metrics);
}
```

### 初始化 PoolChunkList

![在这里插入图片描述](./assets/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0NsYXJlbmNlWmVybw==,size_16,color_FFFFFF,t_70-1684469627027-3.png)

`q000`、`q025`、`q050`、`q075`、`q100` 表示最低内存使用率。如下图所示![在这里插入图片描述](./assets/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0NsYXJlbmNlWmVybw==,size_16,color_FFFFFF,t_70-1684469635422-6.png)

​		任意 PoolChunkList 都有内存使用率的上下限: minUsag、maxUsage。如果使用率超过 maxUsage，那么 PoolChunk 会从当前 PoolChunkList 移除，并移动到下一个PoolChunkList 。同理，如果使用率小于 minUsage，那么 PoolChunk 会从当前 PoolChunkList 移除，并移动到前一个PoolChunkList。
​		每个 PoolChunkList 的上下限都有交叉重叠的部分，因为 PoolChunk 需要在 PoolChunkList 不断移动，如果临界值恰好衔接的，则会导致 PoolChunk 在两个 PoolChunkList 不断移动，造成性能损耗。
​		PoolChunkList 适用于 Chunk 场景下的内存分配，PoolArena 初始化 6 个 PoolChunkList 并按上图首尾相连，形成双向链表，唯独 q000 这个 PoolChunkList 是没有前向节点，是因为当其余 PoolChunkList 没有合适的 PoolChunk 可以分配内存时，会创建一个新的 PoolChunk 放入 pInit 中，然后根据用户申请内存大小分配内存。而在 p000 中的 PoolChunk ，如果因为内存归还的原因，使用率下降到 0%，则不需要放入 pInit，直接执行销毁方法，将整个内存块的内存释放掉。这样，内存池中的内存就有生成/销毁等完成生命周期流程，避免了在没有使用情况下还占用内存。

```java
    // 初始化 PoolChunkList,设置上下限
    q100 = new PoolChunkList<T>(this, null, 100, Integer.MAX_VALUE, chunkSize);
    q075 = new PoolChunkList<T>(this, q100, 75, 100, chunkSize);
    q050 = new PoolChunkList<T>(this, q075, 50, 100, chunkSize);
    q025 = new PoolChunkList<T>(this, q050, 25, 75, chunkSize);
    q000 = new PoolChunkList<T>(this, q025, 1, 50, chunkSize);
    qInit = new PoolChunkList<T>(this, q000, Integer.MIN_VALUE, 25, chunkSize);
```

### 初始化 PoolSubpage[]

​		对于 Small 它拥有四种不同大小的规格，因此 smallSupbagePools 的数组长度为 4，smallSubpagePools[0] 表示 elemSize=512B 的 PoolSubpage 对象的链表，smallSubpagePols[1] 表示 elemSize=1024B 的 PoolSubpages 对象的链表。

​		以此类推，tinySubpagePools 原理一样，只不过划分的粒度（步长）比较少，以 16 的倍数递增。因此，由于 Tiny 大小限制，总共可分为 32 类，因此 tinySubpagePools 数组长度为 32。数组下标所对应的 size 容量不一样，且每个数组都对应一组双向链表。这两个数组用来存储 PoolSubpage 对象且按 PoolSubpage#elemSize 确定索引的位置 index，最后将它们构造双向链表。

## PoolChunkList 源码

```java
package io.netty.buffer;

final class PoolChunkList<T> implements PoolChunkListMetric {
    private static final Iterator<PoolChunkMetric> EMPTY_METRICS = Collections.<PoolChunkMetric>emptyList().iterator();
    // 所属的Arena
    private final PoolArena<T> arena;
    private final PoolChunkList<T> nextList; // 下一个状态
    private final int minUsage; // 状态的最小内存使用率
    private final int maxUsage; // 状态的最大内存使用率
    private final int maxCapacity; // 该状态下的一个Chunk可分配的最大字节数
    private PoolChunk<T> head; // head节点

    // This is only update once when create the linked like list of PoolChunkList in PoolArena constructor.
    // 当在PoolArena构造函数中创建PoolChunkList的类似链接的列表时，这只更新一次。
    private PoolChunkList<T> prevList; // 上一个状态

    // TODO: Test if adding padding helps under contention
    //private long pad0, pad1, pad2, pad3, pad4, pad5, pad6, pad7;

    PoolChunkList(PoolArena<T> arena, PoolChunkList<T> nextList, int minUsage, int maxUsage, int chunkSize) {
        assert minUsage <= maxUsage;
        this.arena = arena;
        this.nextList = nextList;
        this.minUsage = minUsage;
        this.maxUsage = maxUsage;
        // 计算该状态下，一个Chunk块可以分配的最大内存
        maxCapacity = calculateMaxCapacity(minUsage, chunkSize);
    }

    /**
     * Calculates the maximum capacity of a buffer that will ever be possible to allocate out of the {@link PoolChunk}s
     * that belong to the {@link PoolChunkList} with the given {@code minUsage} and {@code maxUsage} settings.
     *
     * 计算缓冲区的最大容量，该缓冲区在给定的{@code minUsage}和{@code maxUsage}设置下，
     * 可以从属于{@link PoolChunkList}的{@link PoolChunk}中分配。
     */
    private static int calculateMaxCapacity(int minUsage, int chunkSize) {
        minUsage = minUsage0(minUsage);

        if (minUsage == 100) {
            // If the minUsage is 100 we can not allocate anything out of this list.
            // 如果minUsage是100，我们就不能从这个列表中分配任何东西。
            return 0;  // Q100 不能再分配
        }

        // Calculate the maximum amount of bytes that can be allocated from a PoolChunk in this PoolChunkList.
        //
        // As an example:
        // - If a PoolChunkList has minUsage == 25 we are allowed to allocate at most 75% of the chunkSize because
        //   this is the maximum amount available in any PoolChunk in this PoolChunkList.
        //
        //计算可从此PoolChunkList中的PoolChunk分配的最大字节数。
        // 例如：-如果PoolChunkList的minUsage==25，我们最多可以分配chunkSize的75%，
        // 因为这是该PoolChunkList中任何PoolChunk中可用的最大数量。
        return  (int) (chunkSize * (100L - minUsage) / 100L);
    }

    void prevList(PoolChunkList<T> prevList) {
        assert this.prevList == null;  // 这个方法只应该在创建时调用一次
        this.prevList = prevList;
    }

    boolean allocate(PooledByteBuf<T> buf, int reqCapacity, int normCapacity) {
        if (normCapacity > maxCapacity) {
            // Either this PoolChunkList is empty or the requested capacity is larger then the capacity which can
            // be handled by the PoolChunks that are contained in this PoolChunkList.
            // 此PoolChunkList为空，或者请求的容量大于此PoolChunk List中包含的PoolChunk可以处理的容量。
            return false;
        }

        // Chunk链表中寻找满足需求的Chunk块
        for (PoolChunk<T> cur = head; cur != null; cur = cur.next) {
            if (cur.allocate(buf, reqCapacity, normCapacity)) {
                if (cur.usage() >= maxUsage) {
                    remove(cur);
                    nextList.add(cur);
                }
                return true;
            }
        }
        return false;
    }

    boolean free(PoolChunk<T> chunk, long handle, ByteBuffer nioBuffer) {
        chunk.free(handle, nioBuffer); // chunk释放占用的内存
        if (chunk.usage() < minUsage) {
            remove(chunk);
            // Move the PoolChunk down the PoolChunkList linked-list.
            // 将PoolChunk向下移动到PoolChunkList链接列表中。
            return move0(chunk);
        }
        return true;
    }


    private boolean move(PoolChunk<T> chunk) {
        assert chunk.usage() < maxUsage;

        if (chunk.usage() < minUsage) {
            // Move the PoolChunk down the PoolChunkList linked-list.
            // 将PoolChunk向下移动到PoolChunkList链接列表中。
            return move0(chunk);
        }

        // PoolChunk fits into this PoolChunkList, adding it here.
        // PoolChunk适合这个PoolChunkList，将其添加到这里。
        add0(chunk);
        return true;
    }

    /**
     * Moves the {@link PoolChunk} down the {@link PoolChunkList} linked-list so it will end up in the right
     * {@link PoolChunkList} that has the correct minUsage / maxUsage in respect to {@link PoolChunk#usage()}.
     *
     * 将｛@link PoolChunk｝向下移动到｛@link PoolChunkList｝链接列表中，
     * 使其最终位于右侧的｛@linkPoolChunkList｝中，该列表相对于｛@LinkPoolChunkusage（）｝具有正确的minUsage maxUsage。
     */
    private boolean move0(PoolChunk<T> chunk) {
        if (prevList == null) {
            // There is no previous PoolChunkList so return false which result in having the PoolChunk destroyed and
            // all memory associated with the PoolChunk will be released.
            // 没有以前的PoolChunkList，因此返回false，这将导致PoolChunk被销毁，
            // 并且与PoolChunk相关的所有内存都将被释放。
            assert chunk.usage() == 0;
            return false;
        }
        return prevList.move(chunk); // 向左移动
    }

    void add(PoolChunk<T> chunk) {
        if (chunk.usage() >= maxUsage) {
            nextList.add(chunk);
            return;
        }
        add0(chunk);
    }

    /**
     * Adds the {@link PoolChunk} to this {@link PoolChunkList}.
     * 将｛@link PoolChunk｝添加到此｛@linkPoolChunkList｝中。
     */
    void add0(PoolChunk<T> chunk) {
        chunk.parent = this;
        if (head == null) {
            head = chunk;
            chunk.prev = null;
            chunk.next = null;
        } else {
            chunk.prev = null;
            chunk.next = head;
            head.prev = chunk;
            head = chunk;
        }
    }

    // 删除一个Chunk节点
    private void remove(PoolChunk<T> cur) {
        if (cur == head) {
            head = cur.next;
            if (head != null) {
                head.prev = null;
            }
        } else {
            PoolChunk<T> next = cur.next;
            cur.prev.next = next;
            if (next != null) {
                next.prev = cur.prev;
            }
        }
    }

    @Override
    public int minUsage() {
        return minUsage0(minUsage);
    }

    @Override
    public int maxUsage() {
        return min(maxUsage, 100);
    }

    private static int minUsage0(int value) {
        return max(1, value);
    }

    @Override
    public Iterator<PoolChunkMetric> iterator() {
        synchronized (arena) {
            if (head == null) {
                return EMPTY_METRICS;
            }
            List<PoolChunkMetric> metrics = new ArrayList<PoolChunkMetric>();
            for (PoolChunk<T> cur = head;;) {
                metrics.add(cur);
                cur = cur.next;
                if (cur == null) {
                    break;
                }
            }
            return metrics.iterator();
        }
    }

    @Override
    public String toString() {
        StringBuilder buf = new StringBuilder();
        synchronized (arena) {
            if (head == null) {
                return "none";
            }

            for (PoolChunk<T> cur = head;;) {
                buf.append(cur);
                cur = cur.next;
                if (cur == null) {
                    break;
                }
                buf.append(StringUtil.NEWLINE);
            }
        }
        return buf.toString();
    }

    void destroy(PoolArena<T> arena) {
        PoolChunk<T> chunk = head;
        while (chunk != null) {
            arena.destroyChunk(chunk);
            chunk = chunk.next;
        }
        head = null;
    }
}

```

## PoolSubpage 源码

```java
package io.netty.buffer;

final class PoolSubpage<T> implements PoolSubpageMetric {

    // 记录当前PoolSubpage的8KB内存块是从哪一个PoolChunk中申请到的
    final PoolChunk<T> chunk;
    // 当前PoolSubpage申请的8KB内存在PoolChunk中memoryMap中的下标索引
    private final int memoryMapIdx;
    // 当前PoolSubpage占用的8KB内存在PoolChunk中相对于叶节点的起始点的偏移量
    private final int runOffset;
    // 当前PoolSubpage的页大小，默认为8KB
    private final int pageSize;
    // 存储当前PoolSubpage中各个内存块的使用情况
    private final long[] bitmap;

    PoolSubpage<T> prev;	// 指向前置节点的指针
    PoolSubpage<T> next;	// 指向后置节点的指针

    boolean doNotDestroy;	// 表征当前PoolSubpage是否已经被销毁了
    int elemSize;	// 表征每个内存块的大小，比如我们这里的就是16
    private int maxNumElems;	// 记录内存块的总个数
    private int bitmapLength;	// 记录总共可使用的bitmap数组的元素的个数
    // 记录下一个可用的节点，初始为0，只要在该PoolSubpage中申请过一次内存，就会更新为-1，
    // 然后一直不会发生变化
    private int nextAvail;
    // 剩余可用的内存块的个数
    private int numAvail;

    // TODO: Test if adding padding helps under contention
    //private long pad0, pad1, pad2, pad3, pad4, pad5, pad6, pad7;

    /** 创建链表头的特殊构造函数 */
    PoolSubpage(int pageSize) {
        chunk = null;
        memoryMapIdx = -1;
        runOffset = -1;
        elemSize = -1;
        this.pageSize = pageSize;
        bitmap = null;
    }

    PoolSubpage(PoolSubpage<T> head, PoolChunk<T> chunk, int memoryMapIdx, int runOffset, int pageSize, int elemSize) {
        this.chunk = chunk;
        this.memoryMapIdx = memoryMapIdx;
        this.runOffset = runOffset;
        this.pageSize = pageSize;	// 初始化当前PoolSubpage总内存大小，默认为8KB
        // 计算bitmap长度，这里pageSize >>> 10其实就是将pageSize / 1024，得到的是8，
        // 从这里就可以看出，无论内存块的大小是多少，这里的bitmap长度永远是8，因为pageSize始终是不变的
        bitmap = new long[pageSize >>> 10];
        // 对其余的属性进行初始化
        init(head, elemSize);
    }

    void init(PoolSubpage<T> head, int elemSize) {
        doNotDestroy = true;
        // elemSize记录了当前内存块的大小
        this.elemSize = elemSize;
        if (elemSize != 0) {
            // 初始时，numAvail记录了可使用的内存块个数，其个数可以通过pageSize / elemSize计算，
            // 我们这里就是8192 / 16 = 512。maxNumElems指的是最大可使用的内存块个数，
            // 初始时其是与可用内存块个数一致的。
            maxNumElems = numAvail = pageSize / elemSize;
            nextAvail = 0;// 初始时，nextAvail是0
            // 这里bitmapLength记录了可以使用的bitmap的元素个数，这是因为，我们示例使用的内存块大小是16，
            // 因而其总共有512个内存块，需要8个long才能记录，但是对于一些大小更大的内存块，比如smallSubpagePools
            // 中内存块为1024字节大小，那么其只有8192 / 1024 = 8个内存块，也就只需要一个long就可以表示，
            // 此时bitmapLength就是8。
            // 这里的计算方式应该是bitmapLength = maxNumElems / 64，因为64是一个long的总字节数，
            // 但是Netty将其进行了优化，也就是这里的maxNumElems >>> 6，这是因为2的6次方正好为64
            bitmapLength = maxNumElems >>> 6;
            // 这里(maxNumElems & 63) != 0就是判断元素个数是否小于64，如果小于，则需要将bitmapLegth加一。
            // 这是因为如果其小于64，前面一步的位移操作结果为0，但其还是需要一个long来记录
            if ((maxNumElems & 63) != 0) {
                bitmapLength ++;
            }

            // 对bitmap数组的值进行初始化
            for (int i = 0; i < bitmapLength; i ++) {
                bitmap[i] = 0;
            }
        }

        // 将当前PoolSubpage添加到PoolSubpage的链表中，也就是最开始图中的链表
        addToPool(head);
    }

    /**
     * Returns the bitmap index of the subpage allocation.
     * 返回子页面分配的位图索引。
     */
    long allocate() {
        if (elemSize == 0) {
            return toHandle(0);
        }

        // 如果当前PoolSubpage没有可用的元素，或者已经被销毁了，则返回-1
        if (numAvail == 0 || !doNotDestroy) {
            return -1;
        }

        // 计算下一个可用的内存块的位置
        final int bitmapIdx = getNextAvail();
        int q = bitmapIdx >>> 6; // 获取该内存块是bitmap数组中的第几号元素
        int r = bitmapIdx & 63; // 获取该内存块是bitmap数组中q号位元素的第多少位
        assert (bitmap[q] >>> r & 1) == 0;
        bitmap[q] |= 1L << r; 	// 将bitmap数组中q号元素的目标内存块位置标记为1，表示已经使用

        // 如果当前PoolSubpage中可用的内存块为0，则将其从链表中移除
        if (-- numAvail == 0) {
            removeFromPool();
        }

        // 将得到的bitmapIdx放到返回值的高32位中
        return toHandle(bitmapIdx);
    }

    /**
     * @return {@code true} if this subpage is in use.
     *         {@code false} if this subpage is not used by its chunk and thus it's OK to be released.
     * 如果这个子页面的区块没有使用，因此可以发布。
     */
    boolean free(PoolSubpage<T> head, int bitmapIdx) {
        if (elemSize == 0) {
            return true;
        }
        // 获取当前需要释放的内存块是在bitmap中的第几号元素
        int q = bitmapIdx >>> 6;
        // 获取当前释放的内存块是在q号元素的long型数的第几位
        int r = bitmapIdx & 63;
        // 将目标位置标记为0，表示可使用状态
        bitmap[q] ^= 1L << r;

        // 设置下一个可使用的数据
        setNextAvail(bitmapIdx);

        // numAvail如果等于0，表示之前已经被移除链表了，因而这里释放后需要将其添加到链表中
        if (numAvail++ == 0) {
            addToPool(head);
            return true;
        }

        // 如果可用的数量小于最大数量，则表示其还是在链表中，因而直接返回true
        if (numAvail != maxNumElems) {
            return true;
        } else {
            // Subpage 未使用 (numAvail == maxNumElems)
            if (prev == next) {
                // 如果此子页面是池中唯一剩下的子页面，请不要删除。
                return true;
            }

            // Remove this subpage from the pool if there are other subpages left in the pool.
            // 如果池中还有其他子页，请从池中删除此子页。
            doNotDestroy = false;
            removeFromPool();
            return false;
        }
    }

    private void addToPool(PoolSubpage<T> head) {
        assert prev == null && next == null;
        prev = head;
        next = head.next;
        next.prev = this;
        head.next = this;
    }

    private void removeFromPool() {
        assert prev != null && next != null;
        prev.next = next;
        next.prev = prev;
        next = null;
        prev = null;
    }

    private void setNextAvail(int bitmapIdx) {
        nextAvail = bitmapIdx;
    }

    private int getNextAvail() {
        int nextAvail = this.nextAvail;
        // 如果是第一次尝试获取数据，则直接返回bitmap第0号位置的long的第0号元素，
        // 这里nextAvail初始时为0，在第一次申请之后就会变为-1，后面将不再发生变化，
        // 通过该变量可以判断是否是第一次尝试申请内存
        if (nextAvail >= 0) {
            this.nextAvail = -1;
            return nextAvail;
        }

        // 如果不是第一次申请内存，则在bitmap中进行遍历获取
        return findNextAvail();
    }

    private int findNextAvail() {
        final long[] bitmap = this.bitmap;
        final int bitmapLength = this.bitmapLength;
        // 这里的基本思路就是对bitmap数组进行遍历，首先判断其是否有未使用的内存是否全部被使用过
        // 如果有未被使用的内存，那么就在该元素中找可用的内存块的位置
        for (int i = 0; i < bitmapLength; i++) {
            long bits = bitmap[i];
            if (~bits != 0) {	// 判断当前long型元素中是否有可用内存块
                return findNextAvail0(i, bits);
            }
        }
        return -1;
    }

    // 入参中i表示当前是bitmap数组中的第几个元素，bits表示该元素的值
    private int findNextAvail0(int i, long bits) {
        final int maxNumElems = this.maxNumElems;
        final int baseVal = i << 6;	// 这里baseVal就是将当前是第几号元素放到返回值的第7~9号位置上

        // 对bits的0~63号位置进行遍历，判断其是否为0，为0表示该位置是可用内存块，从而将位置数据
        // 和baseVal进行或操作，从而得到一个表征目标内存块位置的整型数据
        for (int j = 0; j < 64; j++) {
            if ((bits & 1) == 0) {	// 判断当前位置是否为0，如果为0，则表示是目标内存块
                int val = baseVal | j;	// 将内存快的位置数据和其位置j进行或操作，从而得到返回值
                if (val < maxNumElems) {
                    return val;
                } else {
                    break;
                }
            }
            bits >>>= 1;	// 将bits不断的向右移位，以找到第一个为0的位置
        }
        return -1;
    }

    private long toHandle(int bitmapIdx) {
        return 0x4000000000000000L | (long) bitmapIdx << 32 | memoryMapIdx;
    }

    @Override
    public String toString() {
        final boolean doNotDestroy;
        final int maxNumElems;
        final int numAvail;
        final int elemSize;
        if (chunk == null) {
            // This is the head so there is no need to synchronize at all as these never change.
            doNotDestroy = true;
            maxNumElems = 0;
            numAvail = 0;
            elemSize = -1;
        } else {
            synchronized (chunk.arena) {
                if (!this.doNotDestroy) {
                    doNotDestroy = false;
                    // Not used for creating the String.
                    maxNumElems = numAvail = elemSize = -1;
                } else {
                    doNotDestroy = true;
                    maxNumElems = this.maxNumElems;
                    numAvail = this.numAvail;
                    elemSize = this.elemSize;
                }
            }
        }

        if (!doNotDestroy) {
            return "(" + memoryMapIdx + ": not in use)";
        }

        return "(" + memoryMapIdx + ": " + (maxNumElems - numAvail) + '/' + maxNumElems +
                ", offset: " + runOffset + ", length: " + pageSize + ", elemSize: " + elemSize + ')';
    }

    @Override
    public int maxNumElements() {
        if (chunk == null) {
            // It's the head.
            return 0;
        }

        synchronized (chunk.arena) {
            return maxNumElems;
        }
    }

    @Override
    public int numAvailable() {
        if (chunk == null) {
            // It's the head.
            return 0;
        }

        synchronized (chunk.arena) {
            return numAvail;
        }
    }

    @Override
    public int elementSize() {
        if (chunk == null) {
            // It's the head.
            return -1;
        }

        synchronized (chunk.arena) {
            return elemSize;
        }
    }

    @Override
    public int pageSize() {
        return pageSize;
    }

    void destroy() {
        if (chunk != null) {
            chunk.destroy();
        }
    }
}

```

## PoolChunk

### 概述

​		jemalloc3和jemalloc4的PoolChunk部分有很大的区别，jemalloc4请看 [netty jemalloc4](http://pcat.top/2023/05/18/netty-m-j4/ ) 。

​		Netty底层的内存分配和管理主要由PoolChunk实现，大于16MB的PoolChunk由于不放入内存池管理。PoolChunk内部维护了一棵平衡二叉树，默认由2048个page组成，一个page默认为8KB，整个Chunk默认为16MB。

### 源码

```java
package io.netty.buffer;

/**
 * 从PoolChunk分配PageRunPoolSubpage的算法描述
 *
 * 注释：以下术语对于理解代码很重要
 * > page  - 页面是可以分配的最小内存块单元
 * > chunk - 区块是页面的集合
 * > 在这段代码中，chunkSize = 2^{maxOrder} * pageSize * pageSize
 *
 * 首先，我们分配一个大小=chunkSize的字节数组，当需要创建一个给定大小的ByteBuf时，
 * 我们在字节数组中寻找有足够空位的第一个位置来容纳要求的大小，并返回一个编码这个偏移信息的（长）句柄。
 * 返回一个编码这个偏移信息的（长）句柄，（这个内存段然后被标记为保留，所以它总是被使用。
 * 这个内存段被标记为保留，所以它总是被一个ByteBuf使用，而不是更多
 *
 * 为了简单起见，所有的大小都根据PoolArena#normalizeCapacity方法进行了标准化处理。
 * 这可以确保当我们请求的内存段大小>=pageSize时，规范化的Capacity 等于2的下一个最接近的幂
 *
 * 为了在大块中搜索第一个偏移量，而这个偏移量至少有要求的大小，我们构建一个
 * 完整的平衡二叉树并将其存储在一个数组中（就像堆一样）--memoryMap
 *
 * 这棵树看起来像这样（括号里提到了每个节点的大小）。
 *
 * depth=0        1 node (chunkSize)
 * depth=1        2 nodes (chunkSize/2)
 * ..
 * ..
 * depth=d        2^d nodes (chunkSize/2^d)
 * ..
 * depth=maxOrder 2^maxOrder nodes (chunkSize/2^{maxOrder} = pageSize)
 *
 * depth=maxOrder 是最后一层，叶子由页面组成
 *
 * 有了这个树，在chunkArray中的搜索就可以这样进行：
 * 为了分配一个大小为chunkSize/2^k的内存段，我们搜索高度为k的第一个节点（从左边开始）。
 * 的第一个节点，该节点是未使用的
 *
 * Algorithm:
 * ----------
 * Encode the tree in memoryMap with the notation
 *   memoryMap[id] = x => in the subtree rooted at id, the first node that is free to be allocated
 *   is at depth x (counted from depth=0) i.e., at depths [depth_of_id, x), there is no node that is free
 *
 *  As we allocate & free nodes, we update values stored in memoryMap so that the property is maintained
 *
 * Initialization -
 *   In the beginning we construct the memoryMap array by storing the depth of a node at each node
 *     i.e., memoryMap[id] = depth_of_id
 *
 * Observations:
 * -------------
 * 1) memoryMap[id] = depth_of_id  => it is free / unallocated
 * 2) memoryMap[id] > depth_of_id  => at least one of its child nodes is allocated, so we cannot allocate it, but
 *                                    some of its children can still be allocated based on their availability
 * 3) memoryMap[id] = maxOrder + 1 => the node is fully allocated & thus none of its children can be allocated, it
 *                                    is thus marked as unusable
 *
 * Algorithm: [allocateNode(d) => we want to find the first node (from left) at height h that can be allocated]
 * ----------
 * 1) start at root (i.e., depth = 0 or id = 1)
 * 2) if memoryMap[1] > d => cannot be allocated from this chunk
 * 3) if left node value <= h; we can allocate from left subtree so move to left and repeat until found
 * 4) else try in right subtree
 *
 * Algorithm: [allocateRun(size)]
 * ----------
 * 1) Compute d = log_2(chunkSize/size)
 * 2) Return allocateNode(d)
 *
 * Algorithm: [allocateSubpage(size)]
 * ----------
 * 1) use allocateNode(maxOrder) to find an empty (i.e., unused) leaf (i.e., page)
 * 2) use this handle to construct the PoolSubpage object or if it already exists just call init(normCapacity)
 *    note that this PoolSubpage object is added to subpagesPool in the PoolArena when we init() it
 *
 * Note:
 * -----
 * In the implementation for improving cache coherence,
 * we store 2 pieces of information depth_of_id and x as two byte values in memoryMap and depthMap respectively
 *
 * memoryMap[id]= depth_of_id  is defined above
 * depthMap[id]= x  indicates that the first node which is free to be allocated is at depth x (from root)
 */
final class PoolChunk<T> implements PoolChunkMetric {

    private static final int INTEGER_SIZE_MINUS_ONE = Integer.SIZE - 1;

    // 标识当前Chunk属于哪个Arena
    final PoolArena<T> arena;
    // 实际16MB内存块，如果是Direct则为JDKByteBuffer，如果是Heap则为byte数组
    final T memory;
    // 忽略 内存对齐 认为是0
    final int offset;
    // 树，初始值与depthMap一样
    private final byte[] memoryMap;
    // 节点 - 节点深度
    private final byte[] depthMap;
    // 分配Subpage集合（如果Chunk没有分配过小于等于4KB的内存，这里不会有Subpage实例）
    private final PoolSubpage<T>[] subpages;
    // -8192 掩码，用于判断分配内存是否大于8k，x&-8192 != 0 代表超过8k
    private final int subpageOverflowMask;
    // 8192 页大小
    private final int pageSize;
    // log2(页大小) = 13
    private final int pageShifts;
    // 树深度 = 11
    private final int maxOrder;
    // chunk大小 16MB
    private final int chunkSize;
    // log2(chunk大小) = 24
    private final int log2ChunkSize;
    // Subpage数组大小 = 2048 = 16MB/8KB
    private final int maxSubpageAllocs;
    // 标记位 = 树深度 + 1 = 11 + 1 = 12
    private final byte unusable;
    // 缓存ByteBuffer，减少New对象和GC
    private final Deque<ByteBuffer> cachedNioBuffers;
    // 剩余可分配字节数 = 16MB - 已分配字节数
    int freeBytes;
    // 表示目前在哪个PoolChunkList中
    PoolChunkList<T> parent;
    // 前驱节点
    PoolChunk<T> prev;
    // 后驱节点
    PoolChunk<T> next;

    final boolean unpooled;


    // TODO: Test if adding padding helps under contention
    //private long pad0, pad1, pad2, pad3, pad4, pad5, pad6, pad7;

    PoolChunk(PoolArena<T> arena, T memory, int pageSize, int maxOrder, int pageShifts, int chunkSize, int offset) {
        unpooled = false;
        this.arena = arena;
        this.memory = memory;
        this.pageSize = pageSize;
        this.pageShifts = pageShifts;
        this.maxOrder = maxOrder;
        this.chunkSize = chunkSize;
        this.offset = offset;
        unusable = (byte) (maxOrder + 1);
        log2ChunkSize = log2(chunkSize);
        subpageOverflowMask = ~(pageSize - 1);
        freeBytes = chunkSize;

        assert maxOrder < 30 : "maxOrder should be < 30, but is: " + maxOrder;
        maxSubpageAllocs = 1 << maxOrder;

        // 创建树
        memoryMap = new byte[maxSubpageAllocs << 1];
        depthMap = new byte[memoryMap.length];
        int memoryMapIndex = 1;
        for (int d = 0; d <= maxOrder; ++ d) { // 一次向下移动一级树
            int depth = 1 << d;
            for (int p = 0; p < depth; ++ p) {
                // 在每个级别中，从左到右遍历并将值设置为子树的深度
                memoryMap[memoryMapIndex] = (byte) d;
                depthMap[memoryMapIndex] = (byte) d;
                memoryMapIndex ++;
            }
        }
        // 一个2048个元素的空数组
        subpages = newSubpageArray(maxSubpageAllocs);
        cachedNioBuffers = new ArrayDeque<ByteBuffer>(8);
    }

    /** Creates a special chunk that is not pooled. */
    PoolChunk(PoolArena<T> arena, T memory, int size, int offset) {
        unpooled = true;
        this.arena = arena;
        this.memory = memory;
        this.offset = offset;
        memoryMap = null;
        depthMap = null;
        subpages = null;
        subpageOverflowMask = 0;
        pageSize = 0;
        pageShifts = 0;
        maxOrder = 0;
        unusable = (byte) (maxOrder + 1);
        chunkSize = size;
        log2ChunkSize = log2(chunkSize);
        maxSubpageAllocs = 0;
        cachedNioBuffers = null;
    }

    @SuppressWarnings("unchecked")
    private PoolSubpage<T>[] newSubpageArray(int size) {
        return new PoolSubpage[size];
    }

    @Override
    public int usage() {
        final int freeBytes;
        synchronized (arena) {
            freeBytes = this.freeBytes;
        }
        return usage(freeBytes);
    }

    private int usage(int freeBytes) {
        if (freeBytes == 0) {
            return 100;
        }

        int freePercentage = (int) (freeBytes * 100L / chunkSize);
        if (freePercentage == 0) {
            return 99;
        }
        return 100 - freePercentage;
    }

    boolean allocate(PooledByteBuf<T> buf, int reqCapacity, int normCapacity) {
        final long handle;
        if ((normCapacity & subpageOverflowMask) != 0) { // >= pageSize即Normal请求
            handle =  allocateRun(normCapacity);
        } else {
            // Tiny和Small请求
            handle = allocateSubpage(normCapacity);
        }

        if (handle < 0) {
            return false;
        }
        ByteBuffer nioBuffer = cachedNioBuffers != null ? cachedNioBuffers.pollLast() : null;
        initBuf(buf, nioBuffer, handle, reqCapacity);
        return true;
    }

    /**
     * Update method used by allocate
     * This is triggered only when a successor is allocated and all its predecessors
     * need to update their state
     * The minimal depth at which subtree rooted at id has some free space
     *
     * allocate使用的Update方法只有当一个后继被分配并且它的所有前置都需要更新它们的状态时，
     * 才会触发该方法。根在id处的子树具有一些可用空间的最小深度
     *
     * @param id id
     */
    private void updateParentsAlloc(int id) {
        while (id > 1) {
            int parentId = id >>> 1;
            byte val1 = value(id); // 父节点值
            byte val2 = value(id ^ 1); // 父节点的兄弟（左或者右）节点值
            byte val = val1 < val2 ? val1 : val2; // 取较小值
            setValue(parentId, val);
            id = parentId; // 递归更新
        }
    }

    /**
     * Update method used by free
     * This needs to handle the special case when both children are completely free
     * in which case parent be directly allocated on request of size = child-size * 2
     *
     * free使用的更新方法当两个子项都完全空闲时，需要处理特殊情况，
     * 在这种情况下，根据size＝child - size * 2的请求直接分配父项
     *
     * @param id id
     */
    private void updateParentsFree(int id) {
        int logChild = depth(id) + 1;
        while (id > 1) {
            int parentId = id >>> 1;
            byte val1 = value(id);
            byte val2 = value(id ^ 1);
            // 在第一次迭代中等于log，随后在遍历时从logChild中减少1
            logChild -= 1; // in first iteration equals log, subsequently reduce 1 from logChild as we traverse up

            if (val1 == logChild && val2 == logChild) {
                // 此时子节点均空闲，父节点值高度-1
                setValue(parentId, (byte) (logChild - 1));
            } else {
                // // 此时至少有一个子节点被分配，取最小值
                byte val = val1 < val2 ? val1 : val2;
                setValue(parentId, val);
            }

            id = parentId;
        }
    }

    /**
     * Algorithm to allocate an index in memoryMap when we query for a free node
     * at depth d
     * 当我们在深度d处查询空闲节点时，在memoryMap中分配索引的算法
     *
     * @param d depth
     * @return index in memoryMap
     */
    private int allocateNode(int d) {
        int id = 1;
        // 所有高度<d 的节点 id & initial = 0
        int initial = - (1 << d);
        byte val = value(id); // = memoryMap[id]
        if (val > d) { // 没有满足需求的节点
            return -1;
        }

        // val<d 子节点可满足需求
        // id & initial == 0 高度<d
        while (val < d || (id & initial) == 0) {
            id <<= 1;   // 高度加1，进入子节点
            val = value(id); // = memoryMap[id]
            if (val > d) { // 左节点不满足
                id ^= 1; // 右节点
                val = value(id);
            }
        }

        byte value = value(id);
        assert value == d && (id & initial) == 1 << d : String.format("val = %d, id & initial = %d, d = %d",
                value, id & initial, d);

        // 此时val = d
        setValue(id, unusable); // 找到符合需求的节点并标记为不可用
        updateParentsAlloc(id);  // 更新祖先节点的分配信息
        return id;
    }

    /**
     * 分配页面 run  (>=1)
     *
     * @param normCapacity normalized capacity
     * @return index in memoryMap
     */
    private long allocateRun(int normCapacity) {
        // 计算满足需求的节点的高度
        int d = maxOrder - (log2(normCapacity) - pageShifts);
        // 在该高度层找到空闲的节点
        int id = allocateNode(d);
        if (id < 0) {
            return id;// 没有找到
        }
        freeBytes -= runLength(id); // 分配后剩余的字节数
        return id;
    }

    /**
     * Create / initialize a new PoolSubpage of normCapacity
     * Any PoolSubpage created / initialized here is added to subpage pool in the PoolArena that owns this PoolChunk
     *
     * 创建初始化一个标准容量的新 PoolSubpage 在此初始化的任何创建的
     * PoolSubpage 都将添加到拥有此 PoolChunk 的 PoolArena 中的子页面池
     *
     * @param normCapacity normalized capacity
     * @return index in memoryMap
     */
    private long allocateSubpage(int normCapacity) {
        // Obtain the head of the PoolSubPage pool that is owned by the PoolArena and synchronize on it.
        // 获取PoolArena拥有的PoolSubPage池的头，并在其上进行同步。
        // This is need as we may add it back and so alter the linked-list structure.
        // This is need as we may add it back and so alter the linked-list structure.
        PoolSubpage<T> head = arena.findSubpagePoolHead(normCapacity);
        int d = maxOrder; // subpages are only be allocated from pages i.e.,
        // leaves 子页面只能从页面中分配，即叶子
        // 加锁，分配过程会修改链表结构
        synchronized (head) {
            int id = allocateNode(d);
            if (id < 0) {
                return id; // 叶子节点全部分配完毕
            }

            final PoolSubpage<T>[] subpages = this.subpages;
            final int pageSize = this.pageSize;

            freeBytes -= pageSize;

            // 得到叶子节点的偏移索引，从0开始，即2048-0,2049-1,...
            int subpageIdx = subpageIdx(id);
            PoolSubpage<T> subpage = subpages[subpageIdx];
            if (subpage == null) {
                subpage = new PoolSubpage<T>(head, this, id, runOffset(id), pageSize, normCapacity);
                subpages[subpageIdx] = subpage;
            } else {
                subpage.init(head, normCapacity);
            }
            return subpage.allocate();
        }
    }

    /**
     * Free a subpage or a run of pages
     * When a subpage is freed from PoolSubpage, it might be added back to subpage pool of the owning PoolArena
     * If the subpage pool in PoolArena has at least one other PoolSubpage of given elemSize, we can
     * completely free the owning Page so it is available for subsequent allocations
     *
     * 释放一个子页面或一系列页面当一个子页面从PoolSubpage中释放时，
     * 它可能会被添加回所属PoolArena的子页面池。
     * 如果PoolArena中的子页面库至少有一个其他给定elemSize的PoolSubpage，
     * 我们可以完全释放所属页面，以便后续分配
     *
     * @param handle handle to free
     */
    void free(long handle, ByteBuffer nioBuffer) {
        int memoryMapIdx = memoryMapIdx(handle);
        int bitmapIdx = bitmapIdx(handle);

        if (bitmapIdx != 0) { // free a subpage  // 需要释放subpage
            PoolSubpage<T> subpage = subpages[subpageIdx(memoryMapIdx)];
            assert subpage != null && subpage.doNotDestroy;

            // Obtain the head of the PoolSubPage pool that is owned by the PoolArena and synchronize on it.
            // 获取PoolArena拥有的PoolSubPage池的头，并在其上进行同步。
            // This is need as we may add it back and so alter the linked-list structure.
            // 这是必要的，因为我们可能会将其添加回来，从而改变链表结构。
            PoolSubpage<T> head = arena.findSubpagePoolHead(subpage.elemSize);
            synchronized (head) {
                if (subpage.free(head, bitmapIdx & 0x3FFFFFFF)) {
                    return; // 此时释放了subpage中的一部分内存（即请求的）
                }
            }
        }
        freeBytes += runLength(memoryMapIdx);
        setValue(memoryMapIdx, depth(memoryMapIdx));  // 节点分配信息还原为高度值
        updateParentsFree(memoryMapIdx); // 更新祖先节点的分配信息

        if (nioBuffer != null && cachedNioBuffers != null &&
                cachedNioBuffers.size() < PooledByteBufAllocator.DEFAULT_MAX_CACHED_BYTEBUFFERS_PER_CHUNK) {
            cachedNioBuffers.offer(nioBuffer);
        }
    }

    void initBuf(PooledByteBuf<T> buf, ByteBuffer nioBuffer, long handle, int reqCapacity) {
        int memoryMapIdx = memoryMapIdx(handle);
        int bitmapIdx = bitmapIdx(handle);
        if (bitmapIdx == 0) {
            byte val = value(memoryMapIdx);
            assert val == unusable : String.valueOf(val);
            buf.init(this, nioBuffer, handle, runOffset(memoryMapIdx) + offset,
                    reqCapacity, runLength(memoryMapIdx), arena.parent.threadCache());
        } else {
            initBufWithSubpage(buf, nioBuffer, handle, bitmapIdx, reqCapacity);
        }
    }

    void initBufWithSubpage(PooledByteBuf<T> buf, ByteBuffer nioBuffer, long handle, int reqCapacity) {
        initBufWithSubpage(buf, nioBuffer, handle, bitmapIdx(handle), reqCapacity);
    }

    private void initBufWithSubpage(PooledByteBuf<T> buf, ByteBuffer nioBuffer,
                                    long handle, int bitmapIdx, int reqCapacity) {
        assert bitmapIdx != 0;

        int memoryMapIdx = memoryMapIdx(handle);

        PoolSubpage<T> subpage = subpages[subpageIdx(memoryMapIdx)];
        assert subpage.doNotDestroy;
        assert reqCapacity <= subpage.elemSize;

        buf.init(
            this, nioBuffer, handle,
            runOffset(memoryMapIdx) + (bitmapIdx & 0x3FFFFFFF) * subpage.elemSize + offset,
                reqCapacity, subpage.elemSize, arena.parent.threadCache());
    }

    private byte value(int id) {
        return memoryMap[id];
    }

    private void setValue(int id, byte val) {
        memoryMap[id] = val;
    }

    private byte depth(int id) {
        return depthMap[id];
    }

    private static int log2(int val) {
        // compute the (0-based, with lsb = 0) position of highest set bit i.e, log2
        return INTEGER_SIZE_MINUS_ONE - Integer.numberOfLeadingZeros(val);
    }

    // 得到节点对应可分配的字节，1号节点为16MB-ChunkSize，2048节点为8KB-PageSize
    private int runLength(int id) {
        // represents the size in #bytes supported by node 'id' in the tree
        return 1 << log2ChunkSize - depth(id);
    }

    // 得到节点在chunk底层的字节数组中的偏移量
    // 2048-0, 2049-8K，2050-16K
    private int runOffset(int id) {
        // represents the 0-based offset in #bytes from start of the byte-array chunk
        int shift = id ^ 1 << depth(id);
        return shift * runLength(id);
    }

    // 得到第11层节点的偏移索引，= id - 2048
    private int subpageIdx(int memoryMapIdx) {
        return memoryMapIdx ^ maxSubpageAllocs; // remove highest set bit, to get offset
    }

    private static int memoryMapIdx(long handle) {
        return (int) handle;
    }

    private static int bitmapIdx(long handle) {
        return (int) (handle >>> Integer.SIZE);
    }

    @Override
    public int chunkSize() {
        return chunkSize;
    }

    @Override
    public int freeBytes() {
        synchronized (arena) {
            return freeBytes;
        }
    }

    @Override
    public String toString() {
        final int freeBytes;
        synchronized (arena) {
            freeBytes = this.freeBytes;
        }

        return new StringBuilder()
                .append("Chunk(")
                .append(Integer.toHexString(System.identityHashCode(this)))
                .append(": ")
                .append(usage(freeBytes))
                .append("%, ")
                .append(chunkSize - freeBytes)
                .append('/')
                .append(chunkSize)
                .append(')')
                .toString();
    }

    void destroy() {
        arena.destroyChunk(this);
    }
}

```

## 再讲池化内存分配
